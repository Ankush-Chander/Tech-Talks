{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faa5cc74-7361-4b8f-922c-147e09bc823f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Introduction to Sentence Transformers\n",
    "By Ankush Chander"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a088c3-a99c-41c5-84fe-719995385e72",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Problem statement\n",
    "\n",
    "Sentence pair regression: Given two sentences, generate a numeric value based on the use case.  \n",
    "\n",
    "- **Common use cases**:\n",
    "1. Semantic search: Given a query, find out most similar documents from the corpus.\n",
    "2. Paraphrase mining:  Finding paraphrases (texts with identical / similar meaning) in a large corpus of sentences./5555\n",
    "3. Translated Sentence mining: Bitext mining describes the process of finding parallel (translated) sentence pairs in monolingual corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba24270-250a-44dd-ba50-8c245f0fc0cf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## BERT model \n",
    "\n",
    "BERT(Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "- Encoder only model\n",
    "- Trains on both left as well as right context across all the layers\n",
    "- Two tasks:\n",
    "  - Masked Language modelling(MLM)\n",
    "  - Next word prediction(NSP)\n",
    "- Architecture:\n",
    "   - BERT-BASE (Layers=12, Hidden layer dimensions=768, Attention heads=12, Total Parameters=110M)\n",
    "   - BERT-LARGE (Layers=24, Hidden layer dimensions=1024, Attention heads=16, Total Parameters=340M).\n",
    "- Pretrained on Wikipedia and BooksCorpus\n",
    "- Task specific finetuning using different corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57186f7b-0066-4112-aa10-f2d085d788c2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "![Bert architecture](img/sbert_talk/bert_architecture.png)\n",
    "\n",
    "Picture credits: [Bert paper](https://aclanthology.org/N19-1423.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154f179a-bb6b-4f98-8b6d-4e86d01aa3c8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Sentence embeddings(timeline)\n",
    "\n",
    "\n",
    "| Year | Model/Technique                     | Description                                                                                                                                                                                                             |\n",
    "| ---- | ----------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| 2013 | Word2Vec<br>(Mikolov et al.)        | it uses skip-gram and CBOW models to create dense word vectors.                                                                                                                                                         |\n",
    "| 2014 | GloVe<br>(Pennington et al)         | uses global word-word co-occurrence statistics to create word embeddings.                                                                                                                                               |\n",
    "| 2015 | Doc2Vec<br>(Le and Mikolov)                             | An extension of Word2Vec for creating paragraph or document embeddings.                                                                                                                               |\n",
    "| 2016 | Skip-Thought Vectors<br>(Kiros et al.) | extends the skip-gram model to sentences, capturing semantic meaning over larger contexts.                                                                                                                              |\n",
    "| 2017 | InferSent<br>(Conneau et al., 2017) | uses labeled data of the Stanford Natural Language Inference dataset (Bowman et al., 2015) and the Multi- Genre NLI dataset (Williams et al., 2018) to train a siamese BiLSTM network with max-pooling over the output. |\n",
    "| 2018 | BERT<br>(Devlin et al.)             | It uses a transformer architecture to create deeply contextualized word embeddings.                                                                                                                                     |\n",
    "| 2020 | Sentence-BERT<br>(Reimers et al.)      | It fine-tunes BERT to produce semantically meaningful sentence embeddings.                                                                                                                                              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f227135-0025-4fc4-b4c9-4b9947d91943",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Bi-encoder vs Cross encoder\n",
    "\n",
    "|                       | Bi-Encoder                                                                              | Cross-Encoder                                                                                  |\n",
    "| --------------------- | --------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------- |\n",
    "| **How does it work?** | Calculates embeddings of query as well as document and then measures distance.          | Passes both query as well as document to model as input and ask to generate score between 0-1. |\n",
    "| **Pros**              | - Similarity calculation is fast as embeddings calculated once for all query/docs. O(n) | Generates better score as model pays attention to both query and document at the same time     |\n",
    "|                       | - ==Faster computation== for large datasets                                             | Suitable for higher accuracy over small documents                                              |\n",
    "| **Cons**              | - Does not produce similarity scores for pairs                                          | - Does not produce sentence embeddings                                                         |\n",
    "|                       | - Cannot score pre-defined pairs simultaneously                                         | - Cannot pass individual sentences                                                             |\n",
    "|                       | Does not necessarily works well for asymetric search                                    | Works well for asymetric search due to attention mechanism                                     |\n",
    "| **When to use**       | - Generate candidate documents                                                          | - Rerank small set of candidate documents.                                                     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5632022-1322-4c5f-b3c9-5d7a9d4bc69e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Bi-encoder vs Cross encoder\n",
    "\n",
    "![BiEncoder](https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/Bi_vs_Cross-Encoder.png)\n",
    "\n",
    "Credit: [Docs: Sentence Bert](https://www.sbert.net) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2197efc-442c-4d93-b148-fd579b1f8530",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3030cef2-3c57-490a-bc9e-5798443382a2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Semantic Search\n",
    "Given a query find out documents from corpus which are most semantically(meaningwise not just term wise) similar to the query. \n",
    "\n",
    "| Criteria                 | Symmetric Search                                                                                 | Asymmetric Search                                                                                                                                               |\n",
    "| ------------------------ | ------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| Query and Corpus Length  | Query and entries in the corpus are of about the same length and have the same amount of content | Short query (like a question or some keywords) and longer paragraph answering the query                                                                         |\n",
    "| Example                  | *Query:* \"How to learn Python online?\"<br>*Expected Document:* \"How to learn Python on the web?\" | *Query:* \"What is Python\"<br>*Expected Document:* \"Python is an interpreted, high-level and general-purpose programming language. Python’s design philosophy …\" |\n",
    "| Related Training Example | Quora Duplicate Questions                                                                        | MS MARCO                                                                                                                                                        |\n",
    "| Suitable Models          | Pre-Trained Sentence Embedding Models                                                            | Pre-Trained MS MARCO Models                                                                                                                                     |\n",
    "\n",
    "Based on use-case model should be chosen wisely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73db5c3f-20ff-4e7e-bad3-08d00a2a2774",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Approximate Nearest Neighbor\n",
    "Searching a large corpus with millions of embeddings can be time-consuming if exact nearest neighbor search is used (like it is used by util.semantic_search).\n",
    "\n",
    "Approximate Nearest Neighbor (ANN) can be helpful when dealing with large datasets.\n",
    "- Data is partitioned into smaller fractions of similar embeddings.\n",
    "This index can be searched efficiently, allowing retrieval of the embeddings with the highest similarity (the nearest neighbors) within milliseconds, even with millions of vectors.\n",
    "- ANN methods typically have one or more parameters to tune, determining the recall-speed trade-off:\n",
    "- Three popular libraries for ANN: Annoy, FAISS, and hnswlib."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30944553-da0d-46b3-bccf-8ce74c3da76b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Retrieve and rerank\n",
    "Semantic search is very efficient in terms of computation, however can lead to noisy results.\n",
    "For complex search tasks, for example question answering retrieval, the search can significantly be improved by using Retrieve & Re-Rank.\n",
    "\n",
    "\n",
    "\n",
    "![Retrieve and Rerank](https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/InformationRetrieval.png)\n",
    "\n",
    "Credit: [Docs: Sentence Bert](https://www.sbert.net) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74879c3a-30f7-4a77-9dbf-ab0907fee149",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "106d137e-6180-453d-be48-6a8aa77cefde",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33887/3474522137.py:1: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 384)\n",
      "tensor([[1.0000, 0.6660, 0.1046],\n",
      "        [0.6660, 1.0000, 0.1411],\n",
      "        [0.1046, 0.1411, 1.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankush/workplace/venvs/all_nlp_env/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:439: UserWarning: 1Torch was not compiled with memory efficient attention. (Triggered internally at ../aten/src/ATen/native/transformers/hip/sdp_utils.cpp:505.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm, trange\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Load a pretrained Sentence Transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# The sentences to encode\n",
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\",\n",
    "]\n",
    "\n",
    "# 2. Calculate embeddings by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings.shape)\n",
    "# [3, 384]\n",
    "\n",
    "# 3. Calculate the embedding similarities\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities)\n",
    "# tensor([[1.0000, 0.6660, 0.1046],\n",
    "#         [0.6660, 1.0000, 0.1411],\n",
    "#         [0.1046, 0.1411, 1.0000]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af7fb0f-b6cb-4362-9fab-62ace1af7bf8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474a4424-a7da-460d-916f-42c3b2e0c50a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Key datasets\n",
    "1. [SNLI(Stanford Natural Language Inference)](https://paperswithcode.com/dataset/snli) consists of 570k sentence-pairs manually labeled as entailment, contradiction, and neutral.\n",
    "2. [The MS MARCO (Microsoft MAchine Reading Comprehension)](https://paperswithcode.com/dataset/ms-marco) is a collection of datasets focused on deep learning in search. The first dataset was a question answering dataset featuring 100,000 real Bing questions and a human generated answer.\n",
    "3. [Multi-Genre Natural Language Inference (MultiNLI)](https://huggingface.co/datasets/nyu-mll/multi_nli) corpus is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information.\n",
    "\n",
    "Browse [https://huggingface.co/datasets?other=sentence-transformers](https://huggingface.co/datasets?other=sentence-transformers) to find training datasets that might be useful for your tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2441b672-e9f7-4e55-8ed5-e899e396aefb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Loss functions\n",
    "\n",
    "| Inputs                                  | Labels                         | Appropriate Loss Functions                                                                                                                                                                                                                             |\n",
    "| --------------------------------------- | ------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| `(sentence_A, sentence_B) pairs`        | `class`                        | [`SoftmaxLoss`](https://www.sbert.net/docs/package_reference/sentence_transformer/losses.html#softmaxloss)                                                                                                                                             |\n",
    "| `(anchor, positive/negative) pairs`     | `1 if positive, 0 if negative` | [`ContrastiveLoss`](https://www.sbert.net/docs/package_reference/sentence_transformer/losses.html#contrastiveloss)  <br>[`OnlineContrastiveLoss`](https://www.sbert.net/docs/package_reference/sentence_transformer/losses.html#onlinecontrastiveloss) |\n",
    "| `(sentence_A, sentence_B) pairs`        | `float similarity score`       | [`CoSENTLoss`](https://www.sbert.net/docs/package_reference/sentence_transformer/losses.html#cosentloss)  <br>[`CosineSimilarityLoss`](https://www.sbert.net/docs/package_reference/sentence_transformer/losses.html#cosinesimilarityloss)             |\n",
    "| `(anchor, positive, negative) triplets` | `none`                         | [`TripletLoss`](https://www.sbert.net/docs/package_reference/sentence_transformer/losses.html#tripletloss)                                                                                                                                             |\n",
    "\n",
    "\n",
    "Full list of loss functions can be explored here: [Loss overview](https://www.sbert.net/docs/sentence_transformer/loss_overview.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9285e53b-e65c-420a-85c2-6b554db337fe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## How to train\n",
    "1. Appropriate dataset to one of the acceptable formats.\n",
    "2. Choose loss function consistent with dataset format\n",
    "3. Choose evaluation method consistent with the task\n",
    "4. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ec04767-1b23-4e46-9e71-fac00f164f0e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankush/workplace/os_repos/sentence-transformers/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "No sentence-transformers model found with name microsoft/mpnet-base. Creating a new one with mean pooling.\n",
      "Some weights of MPNetModel were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['mpnet.pooler.dense.bias', 'mpnet.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9480]])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, losses\n",
    "from datasets import Dataset\n",
    "\n",
    "model = SentenceTransformer(\"microsoft/mpnet-base\")\n",
    "\n",
    "embeddings = model.encode([\"It's nice weather outside today.\", \"It's quite rainy, sadly.\"])\n",
    "before_similarity = model.similarity(embeddings[0], embeddings[1])\n",
    "print(before_similarity)\n",
    "\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"anchor\": [\"It's nice weather outside today.\", \"He drove to work.\"],\n",
    "    \"positive\": [\"It's so sunny.\", \"He took the car to the office.\"],\n",
    "    \"negative\": [\"It's quite rainy, sadly.\", \"She walked to the store.\"],\n",
    "})\n",
    "loss = losses.TripletLoss(model=model)\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    loss=loss,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# save model\n",
    "model.save(\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f6505d9-b229-4891-8fee-502cc9e4d640",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8304]])\n"
     ]
    }
   ],
   "source": [
    "# use saved model\n",
    "model = SentenceTransformer(\"my_model\")\n",
    "embeddings = model.encode([\"It's nice weather outside today.\", \"It's quite rainy, sadly.\"])\n",
    "after_similarity = model.similarity(embeddings[0], embeddings[1])\n",
    "print(after_similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d34ab66-c539-4864-b240-ea5446e304b4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Appropriating datasets (examples)\n",
    "\n",
    "1. Abstract analyzer: Given a research paper abstract, classify sentences into categories: research_context, problem_statement, approach, results.\n",
    "  - single sentences |\tclass\n",
    "  - (anchor, positive) pairs | None\n",
    "  - (anchor, positive, negative) triplets\n",
    "2. RAG on your research papers:\n",
    " - (para1, para2, class) 1 if paragraphs from same section of the paper, else 0\n",
    " - (para1, para2, class) 1 if paragraphs from same paper of the paper, else 0\n",
    " - (chunk 1, chunk2, class) 1 if chunk2 is followed by the chunk 1, else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b978862e-6ba2-46f8-a3b3-ff0216770b78",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# References\n",
    "1. [Paper: Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://doi.org/10.18653/v1/d19-1410)\n",
    "2. [Docs: Sentence Transformers](https://www.sbert.net/index.html)\n",
    "3. [Paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://doi.org/10.48550/arxiv.1810.04805)\n",
    "4. [Huggingface - Sentence Transformers](https://huggingface.co/sentence-transformers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
