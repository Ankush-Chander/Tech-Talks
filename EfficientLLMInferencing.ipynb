{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1411cf5-4c92-47b3-9209-8deff50e3d17",
   "metadata": {
    "editable": false,
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Efficient LLM serving\n",
    "By Ankush Chander"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9533fd5-2ad4-4aac-9dd8-6c311d6c7c4d",
   "metadata": {
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Transformers:quick recap\n",
    "<!-- ![](img/llm_serving/decoder_only_model_flow.png)   -->\n",
    "<img src=\"img/llm_serving/decoder_only_model_flow.png\" width=\"650\" height=\"650\">\n",
    "\n",
    "[Image credits: Decoder only Large Language Models (LLM) for text generation - a primer](https://www.linkedin.com/pulse/decoder-only-large-language-models-llm-text-generation-nikhil-goel/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae5e7c7-1a9a-4f5c-9708-44599906136a",
   "metadata": {
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## LLM hardware requirements\n",
    "1. **Model size vs GPU memory:**\n",
    "  Typically, LLM are trained with full- or half-precision floating point numbers (float32 and float16). One float16 has 16 bits, or 2 bytes, and so one billion parameters require 2 gigabytes. You can add 25% more for memory overhead, 50% more for ideal scenario(large batch decoding)\n",
    "    \n",
    "| Model Size(x)<br> In billions         | Minimum GPU memory<br> 2\\*x\\*(1 + .25)| Recommended GPU memory<br> 2\\*x\\*(1 + .5) |\n",
    "| ------------------ | ----------------------- | --------------------------- |\n",
    "| 1B  | 2.5GB          | 3GB           |\n",
    "| 7B  | 17.5GB          | 21GB                |\n",
    "| 13B  | 32.5GB          | 39GB                |\n",
    "| 70B  | 175GB          | 210GB                |\n",
    "   \n",
    "    \n",
    "\n",
    "2. **CPU vs GPU memory:**\n",
    "   Good rule of thumb is to have CPU RAM twice as much as the GPU VRAM \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09fcb80-1d3e-471f-91c4-f729cc000711",
   "metadata": {
    "editable": false,
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Key metrics\n",
    "1. **Time To First Token (TTFT)**: *How quickly users start seeing the model's output after entering their query.* Low waiting times for a response are essential in real-time interactions, but less important in offline workloads. This metric is driven by the time required to process the prompt and then generate the first output token.\n",
    "2. **Time Per Output Token (TPOT)**: *Time to generate an output token for *each* user that is querying our system.* This metric corresponds with how each user will perceive the \"speed\" of the model. For example, a TPOT of 100 milliseconds/tok would be 10 tokens per second per user, or ~450 words per minute, which is faster than a typical person can read.\n",
    "3. **Latency**: The overall time it takes for the model to generate the full response for a user. Overall response latency can be calculated using the previous two metrics: latency = *(TTFT)* + *(TPOT)* \\* (the number of tokens to be generated).\n",
    "4. **Throughput**: The number of output tokens per second an inference server can generate across all users and requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f8e951-a2fe-489c-accc-4240068fa0c7",
   "metadata": {
    "editable": false,
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Text generation\n",
    "Text generation consist of two phases:\n",
    "1. **Prefill phase**: Input prompt is tokenized and all tokens are processed in parellel.\n",
    "2. **Generation phase**: Next token is generated based on tokens seen so far. Works in an *autoregressive* manner such that output token at one step becomes part of the input in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e151d08-3042-4aed-835f-78118b4f5bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import itertools\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "# add seed to get consistent results\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ee4ea14-041d-421d-8506-cc4cc6c33294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model and tokenizer\n",
    "device = \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "# use gpu\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8aaed1b9-6421-480e-9091-d2ec8e3ffb1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf3240385b904f82a23565f016d0df5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quick brown fox\u001b[31mes are the most common species of fox in the United States. They are\u001b[0m\n",
      "Quick brown fox\u001b[31mes are the most common species of fox in the United States. They are\u001b[0m\n",
      "Quick brown fox jumped on\u001b[31m top of her and started to run.\n",
      "\n",
      "\"I'm sorry,\u001b[0m\n",
      "Quick brown fox jumped\u001b[31m up and down on the ground, and then he jumped up and down on\u001b[0m\n",
      "Quick brown fox jumped\u001b[31m up and down on the ground, and then he jumped up and down on\u001b[0m\n",
      "Quick brown fox jumped\u001b[31m up and down on the ground, and then he jumped up and down on\u001b[0m\n",
      "Quick brown fox\u001b[31mes are the most common species of fox in the United States. They are\u001b[0m\n",
      "Quick brown fox\u001b[31mes are the most common species of fox in the United States. They are\u001b[0m\n",
      "Quick brown fox jumped on the\u001b[31m back of the car and ran away.\n",
      "\n",
      "\"I'm sorry,\u001b[0m\n",
      "Quick brown fox\u001b[31mes are the most common species of fox in the United States. They are\u001b[0m\n",
      "Quick brown fox\u001b[31mes are the most common species of fox in the United States. They are\u001b[0m\n",
      "Quick brown fox\u001b[31mes are the most common species of fox in the United States. They are\u001b[0m\n",
      "sequential generate_text took:14.114291906356812\n"
     ]
    }
   ],
   "source": [
    "# form request queue\n",
    "input_samples = [\n",
    "    (\"Quick brown fox\", 15),\n",
    "    (\"Quick brown fox jumped\", 15),\n",
    "    (\"Quick brown fox jumped on\", 15),\n",
    "    (\"Quick brown fox jumped on the\", 15),\n",
    "]\n",
    "queue_size = 12\n",
    "request_queue = [random.choice(input_samples) for i in range(queue_size)]\n",
    "\n",
    "\n",
    "def generate_token(next_inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**next_inputs)\n",
    "        logits = outputs.logits\n",
    "        last_logits = logits[:, -1, :]\n",
    "        # print(last_logits.shape)\n",
    "        next_idx = last_logits.argmax()\n",
    "        return next_idx\n",
    "\n",
    "\n",
    "def generate_text(text: str, num_tokens):\n",
    "    # tokenize input\n",
    "    # prefill stage\n",
    "    next_inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    next_inputs.to(device)\n",
    "    # print(input_tokens)\n",
    "\n",
    "    # generation phase\n",
    "    generated_tokens = []\n",
    "    for i in range(num_tokens):\n",
    "        next_idx = generate_token(next_inputs)\n",
    "        generated_tokens.append(next_idx)\n",
    "        next_inputs = {\n",
    "            \"input_ids\": torch.cat([next_inputs[\"input_ids\"], next_idx.reshape(1, 1).to(device)], dim=1),\n",
    "            \"attention_mask\": torch.cat([next_inputs[\"attention_mask\"], torch.tensor([[1]]).to(device)], dim=1)\n",
    "        }\n",
    "        # append next_idx to inputs.input_ids\n",
    "    print(f\"{text}\\x1b[31m{tokenizer.decode(generated_tokens)}\\x1b[0m\")\n",
    "\n",
    "\n",
    "s_time = time.time()\n",
    "for text, num_tokens in tqdm(request_queue):\n",
    "    generate_text(text, num_tokens)\n",
    "    # break\n",
    "print(f\"sequential generate_text took:{time.time() - s_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546d5cb2-4175-4841-8dc9-ebc3560b169f",
   "metadata": {
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## KV caching\n",
    "**KV-caching** is a technique to speed up token generation by storing some of the tensors in the attention head for use in subsequent generation steps.\n",
    "\n",
    "\n",
    "**How much memory kv cache takes?** \n",
    "- Formula:Â `2 x precision x layers x dimension x sequence_length x batch`\n",
    "- Elements:\n",
    "- 2 for K and V matrices.\n",
    "- Precision: number of bytes per parameter.\n",
    "- Layers: total number of layers.\n",
    "- Dimension: size of embeddings per layer.\n",
    "- Sequence_length: length of the sequence to generate.\n",
    "- Batch: batch size.\n",
    "\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8237ad15-f332-4548-bf32-66905fe7931d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a3385650b0436aa1b5ea769e22e611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quick brown fox\u001b[31mes are the most common species of fox in the United States. They are\u001b[0m\n",
      "Quick brown fox\u001b[31mes are the most common species of fox in the United States. They are\u001b[0m\n",
      "Quick brown fox jumped on\u001b[31m top of her and started to run.\n",
      "\n",
      "\"I'm sorry,\u001b[0m\n",
      "Quick brown fox jumped\u001b[31m up and down on the ground, and then he jumped up and down on\u001b[0m\n",
      "Quick brown fox jumped\u001b[31m up and down on the ground, and then he jumped up and down on\u001b[0m\n",
      "Quick brown fox jumped\u001b[31m up and down on the ground, and then he jumped up and down on\u001b[0m\n",
      "Quick brown fox\u001b[31mes are the most common species of fox in the United States. They are\u001b[0m\n",
      "Quick brown fox\u001b[31mes are the most common species of fox in the United States. They are\u001b[0m\n",
      "Quick brown fox jumped on the\u001b[31m back of the car and ran away.\n",
      "\n",
      "\"I'm sorry,\u001b[0m\n",
      "Quick brown fox\u001b[31mes are the most common species of fox in the United States. They are\u001b[0m\n",
      "Quick brown fox\u001b[31mes are the most common species of fox in the United States. They are\u001b[0m\n",
      "Quick brown fox\u001b[31mes are the most common species of fox in the United States. They are\u001b[0m\n",
      "kv cached generate_text took:4.975010395050049\n"
     ]
    }
   ],
   "source": [
    "def generate_token_with_past(inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    last_logits = logits[0, -1, :]\n",
    "    next_token_id = last_logits.argmax()\n",
    "    return next_token_id, outputs.past_key_values\n",
    "\n",
    "\n",
    "# use past_key_values\n",
    "def generate_text_with_kv_caching(text: str, num_tokens):\n",
    "    # tokenize input\n",
    "    s_time = time.time()\n",
    "    next_inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    next_inputs.to(device)\n",
    "    generated_words = []\n",
    "    for i in range(num_tokens):            \n",
    "        with torch.no_grad():\n",
    "            next_token_id, past_key_values = generate_token_with_past(next_inputs)\n",
    "            # past_key_values: Tuple of tuple(torch.FloatTensor) \n",
    "            # of length config.n_layers, \n",
    "            # with each tuple having 2 tensors of\n",
    "            # shape (batch_size, num_heads, sequence_length, embed_size_per_head))\n",
    "        generated_words.append(next_token_id)\n",
    "        next_inputs = {\n",
    "            \"input_ids\": next_token_id.reshape((1, 1)),\n",
    "            \"attention_mask\": torch.cat([next_inputs[\"attention_mask\"], torch.tensor([[1]]).to(device)], dim=1),\n",
    "            \"past_key_values\": past_key_values\n",
    "        }\n",
    "        \n",
    "    print(f\"{text}\\x1b[31m{tokenizer.decode(generated_words)}\\x1b[0m\")\n",
    "\n",
    "\n",
    "s_time = time.time()\n",
    "for text, num_tokens in tqdm(request_queue):\n",
    "    generate_text_with_kv_caching(text, num_tokens)\n",
    "    # print(\"\\x1b[31m\\\"red\\\"\\x1b[0m\")\n",
    "    # break\n",
    "print(f\"kv cached generate_text took:{time.time() - s_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759a1481-1121-4264-8dba-d252f7ccfd34",
   "metadata": {
    "editable": true,
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Batching\n",
    "Baching in Large Language Model (LLM) inference refers to the practice of grouping multiple inference requests together to be processed simultaneously, rather than handling each request individually.  \n",
    "**On GPUs**  \n",
    "LLM inference is often memory-IO bound, meaning that the speed at which data can be loaded into the GPU's memory significantly impacts the overall performance. By batching requests, the model parameters are loaded into memory fewer times, thereby reducing the overhead associated with frequent memory accesses.  \n",
    "**On CPUs**  \n",
    "Modern frameworks benifits from batching even on CPU because of vectorization and optimized matrix operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddeb2f3-84e4-43a7-80f7-f75f5a06913a",
   "metadata": {},
   "source": [
    "### batching related bookkeeping\n",
    "**Padding:** When there are multiple inputs in a batch, to make them of same size pad tokens are added to it and same thing is reflected in attention_mask so that they can be ignored accordingly.\n",
    "\n",
    "**position_ids:** In single input without kv caching, we send entire sequence so far as input, hence position is clear. In kv caching  we send only the last generated token as input while remaining part of sequence will be  sent as past_key_values. in this case it\"s not clear to llm which position in the sequence the current input token belongs. hence position_id is sent explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60999d08-5519-494f-b27a-52d9fa3afecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Quick brown fox', 15), ('Quick brown fox jumped', 15), ('Quick brown fox jumped on', 15), ('Quick brown fox jumped on the', 15)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[50256, 50256, 50256, 21063,  7586, 21831],\n",
       "        [50256, 50256, 21063,  7586, 21831, 11687],\n",
       "        [50256, 21063,  7586, 21831, 11687,   319],\n",
       "        [21063,  7586, 21831, 11687,   319,   262]]), 'attention_mask': tensor([[0, 0, 0, 1, 1, 1],\n",
       "        [0, 0, 1, 1, 1, 1],\n",
       "        [0, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# pad on the left so we can append new tokens on the right\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\"\n",
    "\n",
    "print(input_samples)\n",
    "texts = [tup[0] for tup in input_samples]\n",
    "tokenizer(texts, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "880d93dc-9ecd-43cf-9141-f081e7ffccf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.Quick brown fox\u001b[31mes are the most common species of fox in the United States. They are\u001b[0m\n",
      "1.Quick brown fox\u001b[31mes are the most common species of fox in the United States. They are\u001b[0m\n",
      "2.Quick brown fox jumped on\u001b[31m top of her and started to run.\n",
      "\n",
      "\"I'm sorry,\u001b[0m\n",
      "3.Quick brown fox jumped\u001b[31m up and down on the ground, and then he jumped up and down on\u001b[0m\n",
      "4.Quick brown fox jumped\u001b[31m up and down on the ground, and then he jumped up and down on\u001b[0m\n",
      "5.Quick brown fox jumped\u001b[31m up and down on the ground, and then he jumped up and down on\u001b[0m\n",
      "6.Quick brown fox\u001b[31mes are the most common species of fox in the United States. They are\u001b[0m\n",
      "7.Quick brown fox\u001b[31mes are the most common species of fox in the United States. They are\u001b[0m\n",
      "8.Quick brown fox jumped on the\u001b[31m back of the car and ran away.\n",
      "\n",
      "\"I'm sorry,\u001b[0m\n",
      "9.Quick brown fox\u001b[31mes are the most common species of fox in the United States. They are\u001b[0m\n",
      "10.Quick brown fox\u001b[31mes are the most common species of fox in the United States. They are\u001b[0m\n",
      "11.Quick brown fox\u001b[31mes are the most common species of fox in the United States. They are\u001b[0m\n",
      "batch+kv_caching generate_text took:1.3788926601409912\n"
     ]
    }
   ],
   "source": [
    "def generate_batch_tokens_with_past(inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    last_logits = logits[:, -1, :]\n",
    "    # print(last_logits.shape)\n",
    "    next_token_ids = last_logits.argmax(dim=1)\n",
    "    # print(next_token_ids)\n",
    "    return next_token_ids, outputs.past_key_values\n",
    "\n",
    "\n",
    "# use past_key_values\n",
    "def generate_text_with_batching(texts: list, max_tokens=10, show=1):\n",
    "    # tokenize input\n",
    "    next_inputs = tokenizer(texts, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    # print(next_inputs) \n",
    "    # prepare position ids as per batching\n",
    "    attention_mask = next_inputs[\"attention_mask\"]\n",
    "    position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "    position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "    next_inputs[\"position_ids\"] = position_ids\n",
    "    # print(f\"position_ids:{position_ids}\")\n",
    "\n",
    "    generated_words = [[] for i in range(len(texts))]\n",
    "\n",
    "    while max_tokens != 0:\n",
    "        next_token_ids, past_key_values = generate_batch_tokens_with_past(next_inputs)\n",
    "        # past_key_values: Tuple of tuple(torch.FloatTensor) \n",
    "        # of length config.n_layers, \n",
    "        # with each tuple having 2 tensors of\n",
    "        # shape (batch_size, num_heads, sequence_length, embed_size_per_head))\n",
    "        max_tokens -= 1\n",
    "        next_inputs = {\n",
    "            # pass latest generated tokens as input\n",
    "            \"input_ids\": next_token_ids.unsqueeze(-1),\n",
    "            # increment last positions by one and send\n",
    "            \"position_ids\": next_inputs[\"position_ids\"][:, -1].unsqueeze(-1) + 1,\n",
    "            # append 1 to existing attention mask\n",
    "            \"attention_mask\": torch.cat(\n",
    "                [next_inputs[\"attention_mask\"], torch.ones(next_inputs[\"attention_mask\"].shape[0]).unsqueeze(-1)],\n",
    "                dim=1),\n",
    "            \"past_key_values\": past_key_values\n",
    "        }\n",
    "\n",
    "        for i, idx in enumerate(next_token_ids):\n",
    "            generated_words[i].append(idx)\n",
    "    if show:\n",
    "        for i, idx in enumerate(generated_words):\n",
    "            print(f\"{i}.{texts[i]}\\x1b[31m{tokenizer.decode(idx)}\\x1b[0m\")\n",
    "\n",
    "\n",
    "s_time = time.time()\n",
    "texts = [tup[0] for tup in request_queue]\n",
    "max_tokens = 15\n",
    "generate_text_with_batching(texts, max_tokens)\n",
    "print(f\"batch+kv_caching generate_text took:{time.time() - s_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81b6623-1459-4b87-80fe-3b8953ac88f0",
   "metadata": {
    "editable": true,
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Continuos batching\n",
    "Simple batching waits for a batch to finish before processing next set of requests.  \n",
    "**Continuos batching** is able to swap completed requests with new requests at iteration level. It can achieve 10x-20x better throughput than simple batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92e11e47-6dbb-4c7d-9604-18c4d598b0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Jack and Jill', 100), ('Quick brown fox jumped', 15), ('Humpty Dumpty sat', 30), ('Jack and Jill', 100), ('Quick brown fox jumped', 15), ('Humpty Dumpty sat', 30), ('Jack and Jill', 100), ('Quick brown fox jumped', 15), ('Humpty Dumpty sat', 30), ('Jack and Jill', 100), ('Quick brown fox jumped', 15), ('Humpty Dumpty sat', 30)]\n",
      "+0.726Quick brown fox jumped\u001b[31m and down on the ground, and then he jumped up and down on the\u001b[0m\n",
      "+1.658Humpty Dumpty sat\u001b[31m the throne of the House of Commons, and he was a man of great courage and of great courage. He was a man of great courage and of\u001b[0m\n",
      "+1.658Quick brown fox jumped\u001b[31m and down on the ground, and then he jumped up and down on the\u001b[0m\n",
      "+3.327Humpty Dumpty sat\u001b[31m the throne of the House of Commons, and he was a man of great courage and of great courage. He was a man of great courage and of\u001b[0m\n",
      "+4.362Quick brown fox jumped\u001b[31m and down on the ground, and then he jumped up and down on the\u001b[0m\n",
      "+5.899Jack and Jill\u001b[31m who are both in their late teens, are in their late teens.\n",
      "\n",
      "\"I'm not sure if they're going to be able to get married, but I'm sure they're going to be able to get married,\" said Jill.\n",
      "\n",
      "\"I'm not sure if they're going to be able to get married, but I'm sure they're going to be able to get married,\" said Jill.\n",
      "\n",
      "\"I'm not sure if they're going to be able to get\u001b[0m\n",
      "+5.9Jack and Jill\u001b[31m who are both in their late teens, are in their late teens.\n",
      "\n",
      "\"I'm not sure if they're going to be able to get married, but I'm sure they're going to be able to get married,\" said Jill.\n",
      "\n",
      "\"I'm not sure if they're going to be able to get married, but I'm sure they're going to be able to get married,\" said Jill.\n",
      "\n",
      "\"I'm not sure if they're going to be able to get\u001b[0m\n",
      "+6.505Humpty Dumpty sat\u001b[31m the throne of the House of Commons, and he was a man of great courage and of great courage. He was a man of great courage and of\u001b[0m\n",
      "+7.235Quick brown fox jumped\u001b[31m and down on the ground, and then he jumped up and down on the\u001b[0m\n",
      "+7.886Jack and Jill\u001b[31m who are both in their late teens, are in their late teens.\n",
      "\n",
      "\"I'm not sure if they're going to be able to get married, but I'm sure they're going to be able to get married,\" said Jill.\n",
      "\n",
      "\"I'm not sure if they're going to be able to get married, but I'm sure they're going to be able to get married,\" said Jill.\n",
      "\n",
      "\"I'm not sure if they're going to be able to get\u001b[0m\n",
      "+8.061Humpty Dumpty sat\u001b[31m the throne of the House of Commons, and he was a man of great courage and of great courage. He was a man of great courage and of\u001b[0m\n",
      "+9.896Jack and Jill\u001b[31m who are both in their late teens, are in their late teens.\n",
      "\n",
      "\"I'm not sure if they're going to be able to get married, but I'm sure they're going to be able to get married,\" said Jill.\n",
      "\n",
      "\"I'm not sure if they're going to be able to get married, but I'm sure they're going to be able to get married,\" said Jill.\n",
      "\n",
      "\"I'm not sure if they're going to be able to get\u001b[0m\n",
      "kv cached generate_text took:10.063291311264038\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import itertools\n",
    "    \n",
    "# form request queue\n",
    "input_samples = [\n",
    "    (\"Jack and Jill\", 100),\n",
    "    (\"Quick brown fox jumped\", 15),\n",
    "    (\"Humpty Dumpty sat\", 30),\n",
    "]\n",
    "\n",
    "batch_size = 4\n",
    "queue_size = 12\n",
    "\n",
    "request_queue = [input_samples[i%len(input_samples)] for i in range(queue_size)]\n",
    "\n",
    "print(request_queue)\n",
    "\n",
    "\n",
    "# request_queue\n",
    "\n",
    "def decode_tokens(tokens):\n",
    "    return tokenizer.decode(tokens)\n",
    "\n",
    "\n",
    "def filter_batch(next_inputs, incomplete_mask):\n",
    "    \"\"\"\n",
    "    filter out completed requests from inputs corresponding to mask\n",
    "    \"\"\"\n",
    "    next_inputs[\"position_ids\"] = next_inputs[\"position_ids\"][incomplete_mask]\n",
    "    next_inputs[\"input_ids\"] = next_inputs[\"input_ids\"][incomplete_mask]\n",
    "    next_inputs[\"attention_mask\"] = next_inputs[\"attention_mask\"][incomplete_mask]\n",
    "\n",
    "    # past_key_values: Tuple of tuple(torch.FloatTensor) of length config.n_layers, with each tuple having 2 tensors of  shape (batch_size, num_heads, sequence_length, embed_size_per_head))\n",
    "    # filter past_key_values using incomplete_mask\n",
    "    next_inputs[\"past_key_values\"] = [(layer_tup[0][incomplete_mask], layer_tup[1][incomplete_mask]) for layer_tup in\n",
    "                                      next_inputs[\"past_key_values\"]]\n",
    "    return next_inputs\n",
    "\n",
    "\n",
    "def prefill_batch(texts, max_len=None):\n",
    "    \"\"\"\n",
    "    generate key values for the input text for later use \n",
    "    \"\"\"\n",
    "    if max_len is None:\n",
    "        max_len = max([len(text) for text in texts])\n",
    "    # tokenize input with padding according to max_len\n",
    "    next_inputs = tokenizer(texts, max_length=max_len-1, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    # prepare position ids as per batching\n",
    "    attention_mask = next_inputs[\"attention_mask\"]\n",
    "    position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "    position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "    next_inputs[\"position_ids\"] = position_ids\n",
    "    # print(f\"position_ids:{position_ids}\")\n",
    "    input_id_text_map = torch.IntTensor(range(len(texts)))\n",
    "    next_token_ids, past_key_values = generate_batch_tokens_with_past(next_inputs)\n",
    "    next_inputs = {\n",
    "        # pass latest generated tokens as input\n",
    "        \"input_ids\": next_token_ids.unsqueeze(-1),\n",
    "        # increment last positions by one and send\n",
    "        \"position_ids\": next_inputs[\"position_ids\"][:, -1].unsqueeze(-1) + 1,\n",
    "        # append 1 to existing attention mask\n",
    "        \"attention_mask\": torch.cat(\n",
    "            [next_inputs[\"attention_mask\"], torch.ones(next_inputs[\"attention_mask\"].shape[0]).unsqueeze(-1)],\n",
    "            dim=1),\n",
    "        \"past_key_values\": past_key_values\n",
    "    }\n",
    "    return next_inputs\n",
    "\n",
    "\n",
    "def merge_batches(next_inputs, new_next_inputs):\n",
    "    \"\"\"\n",
    "    take existing batch and merge with prefilled batch so that batch remains full\n",
    "    \"\"\"\n",
    "\n",
    "    next_inputs[\"position_ids\"] = torch.cat(\n",
    "        [next_inputs[\"position_ids\"], new_next_inputs[\"position_ids\"]], dim=0)\n",
    "    next_inputs[\"input_ids\"] = torch.cat([next_inputs[\"input_ids\"], new_next_inputs[\"input_ids\"]], dim=0)\n",
    "    \n",
    "    next_inputs[\"attention_mask\"] = torch.cat(\n",
    "        [next_inputs[\"attention_mask\"], new_next_inputs[\"attention_mask\"]], dim=0)\n",
    "    # print(f\"next_inputs['attention_mask']: {next_inputs['attention_mask']}\")\n",
    "\n",
    "    assert len(new_next_inputs['past_key_values']) == len(next_inputs['past_key_values'])\n",
    "    \n",
    "    num_layers = len(next_inputs['past_key_values'])\n",
    "    new_past_key_values= []\n",
    "    for layer_i in range(num_layers):\n",
    "        layerwise_kv= []\n",
    "        for kv_i in (0,1):\n",
    "            layerwise_kv.append(torch.cat([next_inputs[\"past_key_values\"][layer_i][kv_i], new_next_inputs[\"past_key_values\"][layer_i][kv_i]], dim=0))        \n",
    "        # print(f\"layer={layer_i}, layerwise_kv[0].shape={layerwise_kv[0].shape}, layerwise_kv[1].shape={layerwise_kv[1].shape}\")\n",
    "        new_past_key_values.append(layerwise_kv)\n",
    "    next_inputs[\"past_key_values\"] = tuple(new_past_key_values)\n",
    "        \n",
    "    return next_inputs\n",
    "\n",
    "\n",
    "def generate_text_with_continous_batching(request_queue: list, batch_size=5):\n",
    "    \"\"\"\n",
    "    generate text with continous batching   \n",
    "    :param request_queue: list of tuple(text, max_tokens)\n",
    "    :param batch_size: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # initialize_batch\n",
    "\n",
    "    current_texts= [tup[0] for tup in request_queue[:batch_size]] # maintain ongoing texts\n",
    "    max_tokens = [tup[1] for tup in request_queue[:batch_size]] # maintain ongoing max_tokens\n",
    "    generated_words = [[] for i, _ in enumerate(current_texts)] # maintain ongoing generated_words\n",
    "    \n",
    "    \n",
    "    request_queue = request_queue[batch_size:]\n",
    "    \n",
    "    next_inputs = prefill_batch(current_texts)\n",
    "    \n",
    "    s_time = time.time()\n",
    "    batch_capacity = batch_size\n",
    "    \n",
    "    while sum(max_tokens) >= 0:\n",
    "        # track which sequences are complete/incomplete so that complete sequences can be taken out\n",
    "        complete_indices = [i for i, remaining_count in enumerate(list(max_tokens)) if remaining_count == 0]\n",
    "\n",
    "        # mask used to take out complete inputs\n",
    "        incomplete_mask = [remaining_count > 0 for remaining_count in max_tokens]\n",
    "        if complete_indices:\n",
    "            batch_capacity = len(complete_indices)\n",
    "            next_inputs = filter_batch(next_inputs, incomplete_mask)\n",
    "            for i in complete_indices:\n",
    "                print(\n",
    "                    f\"+{round(time.time() - s_time, 3)}{current_texts[i]}\\x1b[31m{tokenizer.decode(generated_words[i])}\\x1b[0m\")\n",
    "            \n",
    "            \n",
    "            current_texts = list(itertools.compress(current_texts, incomplete_mask))\n",
    "            max_tokens = list(itertools.compress(max_tokens, incomplete_mask))\n",
    "            generated_words = list(itertools.compress(generated_words, incomplete_mask))\n",
    "            ###\n",
    "            # Populate missing slots\n",
    "            ###\n",
    "            if request_queue:\n",
    "                new_texts, new_max_tokens = [tup[0] for tup in request_queue[:batch_capacity]], [\n",
    "                    tup[1] for tup in request_queue[:batch_capacity]]\n",
    "                current_texts.extend(new_texts)\n",
    "                max_tokens.extend(new_max_tokens)\n",
    "                generated_words.extend([[] for _ in range(batch_capacity)])\n",
    "            \n",
    "                max_length = next_inputs[\"attention_mask\"].shape[1]\n",
    "    \n",
    "                new_next_inputs = prefill_batch(new_texts, max_length)\n",
    "                # print(f\"next_inputs['input_ids'].shape:{next_inputs['input_ids'].shape}\")\n",
    "                \n",
    "                next_inputs = merge_batches(next_inputs, new_next_inputs)\n",
    "            \n",
    "                # print(f\"next_inputs['input_ids'].shape:{next_inputs['input_ids'].shape}\")\n",
    "                request_queue = request_queue[batch_capacity:]\n",
    "            \n",
    "            if next_inputs[\"input_ids\"].size(0) <= 0:\n",
    "                break\n",
    "        # print(next_inputs)\n",
    "        next_token_ids, past_key_values = generate_batch_tokens_with_past(next_inputs)\n",
    "        max_tokens = [mt - 1 for mt in max_tokens]\n",
    "\n",
    "        next_inputs = {\n",
    "            # pass latest generated tokens as input\n",
    "            \"input_ids\": next_token_ids.unsqueeze(-1),\n",
    "            # increment last positions by one and send\n",
    "            \"position_ids\": next_inputs[\"position_ids\"][:, -1].unsqueeze(-1) + 1,\n",
    "            # append 1 to existing attention mask\n",
    "            \"attention_mask\": torch.cat(\n",
    "                [next_inputs[\"attention_mask\"], torch.ones(next_inputs[\"attention_mask\"].shape[0]).unsqueeze(-1)],\n",
    "                dim=1),\n",
    "            \"past_key_values\": past_key_values\n",
    "        }\n",
    "\n",
    "        # print(f\"generated_words: {generated_words}\")\n",
    "        for i, idx in enumerate(next_token_ids):\n",
    "            generated_words[i].append(idx)\n",
    "\n",
    "\n",
    "s_time = time.time()\n",
    "generate_text_with_continous_batching(request_queue, batch_size)\n",
    "print(f\"kv cached generate_text took:{time.time() - s_time}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764c63bb-0e26-485e-b12c-0fb7275c09d3",
   "metadata": {
    "editable": true,
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Quantization\n",
    "Model quantization is a common way to reduce model hardware requirements. Reducing the precision of the model weights and activations of the model reduces the GPU RAM requirements. For example changing model precision from float16 to int8 halves the size of the VRAM requirements.\n",
    "It also leads to kv cache size reduction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4337ca81-2e13-4192-8626-0db3ab732db3",
   "metadata": {},
   "source": [
    "### Floating point representation\n",
    "\n",
    "| Representation | Mantissa     | Exponent(range of numbers)                          | Sign | Example   |\n",
    "| -------------- | ------------ | --------------------------------------------------- | ---- | --------- |\n",
    "|                | decides the precision with which numbers can be represented | decides the range of number that can be represented |      |           |\n",
    "| FP32           | 23           | 8                                                   | 1    | 3.1415927 |\n",
    "| FP16           | 10           | 5                                                   | 1    | 3.141     |\n",
    "| FP8            | 2            | 5                                                   | 1    | 3         |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc09adb-f8b2-46f8-9fe4-1721a1037510",
   "metadata": {},
   "source": [
    "### Effect of quantization on KV cache requirements\n",
    "\n",
    "\n",
    "\n",
    "| Batch Size | KV cache memory (FP16) | KV cache memory (Int8) |\n",
    "|------------|-----------------------------|----------------------------|\n",
    "| 1          | 0.312 GiB                   | 0.156 GiB                  |\n",
    "| 16         | 5 GiB                       | 2.5 GiB                    |\n",
    "| 32         | 10 GiB                      | 5 GiB                      |\n",
    "| 64         | 20 GiB                      | 10 GiB                     |\n",
    "KV cache size for Llama-2-70B at a sequence length of 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4781ab7b-8b2f-47dd-b0d8-49ee92b83407",
   "metadata": {
    "editable": true,
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_memory_footprint: 486.7002410888672\n"
     ]
    }
   ],
   "source": [
    "# model memory footprint\n",
    "model_memory_footprint = model.get_memory_footprint()/(1024*1024)\n",
    "print(f\"model_memory_footprint: {model_memory_footprint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff895baf-5e63-4283-8da1-59b4f6075bc8",
   "metadata": {
    "editable": true,
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a function to generate text\n",
    "def generate(model, tokenizer, prompt, max_length=20, num_return_sequences=1):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        max_length=max_length,\n",
    "        temperature=1.0,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "    )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ddc104b-d50e-439c-a504-97a66a99f286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankush/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1411: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Quick brown fox jumped on it\\'s hind foot, and it moved outwards, moving toward Ruby.\\n\\nFluid eyes.\\n\\nBut she couldn\\'t even say \"Fluid eyes']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# genrate using original model\n",
    "generate(model,tokenizer, \"Quick brown fox jumped on\", 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53466e62-1d1f-4de6-8309-185519e70083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a quantization function\n",
    "def quantize(t):\n",
    "    min_val, max_val = t.min(), t.max()\n",
    "    scale = (max_val - min_val) / 255\n",
    "    zero_point = min_val\n",
    "    state = (scale, zero_point)\n",
    "    t_quant = (t - min_val) / scale\n",
    "    t_quant = torch.clamp(t_quant, min=0, max=255)\n",
    "\n",
    "    # cast to int8\n",
    "    t_quant = t_quant.type(torch.uint8)\n",
    "\n",
    "    return t_quant, state\n",
    "\n",
    "# define a dequantization function\n",
    "def dequantize(t, state):\n",
    "    scale, min_val = state\n",
    "    # upcast to float\n",
    "    t = t.to(torch.float32)\n",
    "    t = t * scale + min_val\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e13f1cd-98d7-4b58-a932-0758dd1a2873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: tensor([[0.9623, 0.1279, 0.1236],\n",
      "        [0.6088, 0.2284, 0.0868]])\n",
      "quantized:tensor([[255,  11,  10],\n",
      "        [152,  41,   0]], dtype=torch.uint8)\n",
      "scale:(tensor(0.0034), tensor(0.0868))\n",
      "t_recovered: tensor([[0.9623, 0.1246, 0.1211],\n",
      "        [0.6087, 0.2276, 0.0868]])\n"
     ]
    }
   ],
   "source": [
    "# quantize random tensor\n",
    "# initialize random tensor\n",
    "t = torch.rand(2, 3)\n",
    "print(f\"original: {t}\")\n",
    "t_quantized, scale = quantize(t)\n",
    "print(f\"quantized:{t_quantized}\\nscale:{scale}\")\n",
    "\n",
    "t_recovered = dequantize(t_quantized, scale)\n",
    "print(f\"t_recovered: {t_recovered}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3a3873c-2ea5-4225-8af0-f7d88ea88952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130.6750946044922"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quantize entire model\n",
    "def quantize_model(model):\n",
    "    states = {}\n",
    "    for name,param in model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "        param.data , states[name] = quantize(param.data)\n",
    "    return model, states\n",
    "\n",
    "def dequantize_model(q_model, states):\n",
    "    for name ,param in q_model.named_parameters():\n",
    "        param.data = dequantize(param.data, states[name])\n",
    "    return q_model\n",
    "\n",
    "q_model,states = quantize_model(model)\n",
    "q_model.get_memory_footprint()/ (1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d9a52ab-bb24-4b05-a2f1-0bda55b0f251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Quick brown fox jumped on top of them but missed after landing. He ran away crying to his puppy. When he saw him lying naked naked naked naked naked naked naked naked naked naked naked naked naked naked']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate text using recovered model\n",
    "recovered_model = dequantize_model(model, states)\n",
    "q_output = generate(recovered_model,tokenizer, \"Quick brown fox jumped on\", 40)\n",
    "q_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65096c45-52ff-4bc0-9a92-395c3180d3d8",
   "metadata": {},
   "source": [
    "## Adapter based finetuning\n",
    "LoRA, short for Low-Rank Adaptation, is a method designed to efficiently fine-tune large pre-trained models. The intuition behind LoRA stems from the understanding that the vast majority of the parameters in a pre-trained model remain unchanged during fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476a76b2-09ad-430b-a15a-b5c112ffadd0",
   "metadata": {},
   "source": [
    "![Title](img/llm_serving/lora_diagram.png)\n",
    "Image credits: [huggingface](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a714ea-95be-4bc4-a091-753fd985fe86",
   "metadata": {},
   "source": [
    "<img src=\"img/llm_serving/multi_adapter_serving.jpeg\"  width=\"650\" height=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c800e118-3a19-45a9-afbd-4c92ffec65d9",
   "metadata": {
    "editable": true,
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# References\n",
    "1. [DLAI - Efficiently Serving LLMs](https://learn.deeplearning.ai/courses/efficiently-serving-llms)\n",
    "2. [LLM Inference Performance Engineering: Best Practices | Databricks Blog](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices)\n",
    "3. [Hardware for LLMs - by Benjamin Marie](https://kaitchup.substack.com/p/hardware-for-llms)\n",
    "4. [Orca: A Distributed Serving System for Transformer-Based Generative Models | USENIX](https://www.usenix.org/conference/osdi22/presentation/yu)\n",
    "5. [QLoRA: Fine-Tune a Large Language Model on Your GPU](https://kaitchup.substack.com/p/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b)\n",
    "6. [Fundamentals of Data Representation: Floating point numbers - Wikibooks, open books for an open world](https://en.wikibooks.org/wiki/A-level_Computing/AQA/Paper_2/Fundamentals_of_data_representation/Floating_point_numbers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
