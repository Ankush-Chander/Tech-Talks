{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaeb4345-f093-4cb9-9f48-5afb3d6e63ad",
   "metadata": {
    "editable": false,
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Transformers 101\n",
    "By Ankush Chander"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a753b399-d4ac-4bb4-a07b-411ab0e0b3ca",
   "metadata": {
    "editable": false,
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Outline\n",
    "### 1. Context\n",
    "### 2. Encoder decoder framework\n",
    "### 3. Transformer architecture\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed4314-c7eb-43d7-8720-90a749e5d445",
   "metadata": {
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Language modelling\n",
    "Language modeling is a task to predict the next word or character in a sequence of text given the context of the previous words.\n",
    "\n",
    "P(w_n|w_1, w_2, ..., w_n-1) = ?\n",
    "![title](img/transformer-talk/next_word_pred.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3ae84f-8372-4d15-80c5-9e2167630b75",
   "metadata": {
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Evolution of Language models\n",
    "| | Statistical Language models | Neural Language models | Large Language models |\n",
    "| --- | --- | --- | --- |\n",
    "|Pros: | Simple to implement | generalizes well to unseen sequences as it capture semantic relationships | generate coherent and contextually relevant text. |\n",
    "|Cons: | 1.struggle with capturing long-range dependencies. <br> 2. didn\"t capture semantic relationships. <br>Eg: cat sat on a table. vs cat sat on a desk  | Expensive to train | Expensive to train |\n",
    "|Examples: | N-gram models, Hidden Markov Models (HMMs).   | RNNs, LSTMs, GRU | GPT-3, GPT-4, BERT |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbbb2b2-c592-41af-a7ed-63c6aaee16d6",
   "metadata": {
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Word Embeddings\n",
    "The word embedding is a real-valued vector that encodes the meaning of the word in such a way that the words that are closer in the vector space are expected to be similar in meaning.\n",
    "\n",
    "**Key characteristics:**  \n",
    "*Continuous Vector Space:* Each word is represented by a vector in a continuous multi-dimensional space. The position of a word's vector in this space reflects its semantic meaning and relationships with other words.  \n",
    "*Semantic Similarity:* Similar words in meaning are represented by vectors that are closer together in the vector space. For example, words like \"king\" and \"queen\" would have vectors that are closer in proximity.  \n",
    "*Dimensionality:* The dimensionality of word embeddings is a hyperparameter that can be adjusted. Commonly used dimensions range from 50 to 300, but the optimal dimensionality may depend on the specific task and the size of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9a3c4e-ae15-4145-99b0-a6c2b05f4aab",
   "metadata": {
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<img src=\"img/transformer-talk/word_vector.jpeg\" width=\"650\" height=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7f171a-ebd5-472c-ba23-8426c6e6fd16",
   "metadata": {
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Embedding layer\n",
    "**Embedding layer** is the layer in neural NLP models that maps words to their corresponding embeddings. Words usually are represented as *one-hot vectors* of vocab_size. Embedding layer transforms one-hot vectors to corresponding embeddings.\n",
    "Sometimes instead of training the embeddings we can reuse embeddings generated by someone else. In that case it\"s called **pretrained-embeddings**.  \n",
    "<img src=\"img/transformer-talk/embedding_layer.png\" width=\"650\" height=\"650\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5e1ccd-8721-4dd2-b8af-57f302910fa8",
   "metadata": {
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Sequence modelling\n",
    "**Sequence models** are the machine learning models that input or output sequences of data. Sequential data includes text streams, audio clips, video clips, time-series data and etc.  \n",
    "Eg:  \n",
    "Sentiment analysis (text => positive/negative/neutral)  \n",
    "Document summarization (document => summary)  \n",
    "Machine translation (source_language_text => target_language_text)  \n",
    "Text generation (text_so_far => next_word)\n",
    "\n",
    "When the input and desired output are both sequences, it is known as **sequence to sequence models**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4c3f0b-47a7-4c71-b9c2-5b3699f554ba",
   "metadata": {
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Encoder-decoder architecture\n",
    "The encoder-decoder architecture is an ML architecture that is widely used in NLP tasks such as machine translation, text summarization, and language generation.  \n",
    "The **encoder** takes a variable-length sequence as input and transforms it into a state with a fixed shape (**thought vector**) and the **decoder** maps the encoded state of a fixed shape to an output sequence.  \n",
    "![title](img/transformer-talk/encoder_decoder_architecture.png)  \n",
    "*Image Credits: [Illustrated Transformers](https://jalammar.github.io/illustrated-transformer/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f002b5f-aada-4267-a710-54eb832da350",
   "metadata": {
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Variations of encoder-decoder architectures:\n",
    "| | Encoder-Decoder models | Encoder only models | Decoder only models |\n",
    "| --- | --- | --- | --- |\n",
    "| Analogy: | ![title](img/transformer-talk/encoder_decoder_meme.png)   | ![title](img/transformer-talk/encoder_only_meme.jpg) | ![title](img/transformer-talk/decoder_only_meme.jpg)|\n",
    "|Use cases: | sequence to sequence tasks like machine translation, document summarization | Embedding tasks, transfer learning for downstream tasks like classification | Language generation, completion, and other generative tasks. |\n",
    "|Training objective: | Trained to minimize the difference between the predicted and target output sequences.  | Pre-trained on unsupervised tasks like language modeling, masked language modeling, etc.  | Pre-trained for generative tasks, often using autoregressive language modeling. |\n",
    "|Examples: | T5(Text-to-Text Transfer Transformer), BART   | BERT(Bidirectional Encoder Representations from Transformers) | GPT family |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f664142-9fa6-43be-8915-14f351f2af3d",
   "metadata": {
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Attention mechanism\n",
    "The attention mechanism is a powerful technique in machine learning that allows a model to focus on specific parts of the input by assigning different weights to them based on their relevance to the task at hand.\n",
    "### Scaled dot-product attention\n",
    "![Attention mechanism](https://files.readme.io/ab64790-image.png)  \n",
    "*Image Credits: [The Attention Mechanism- Cohere](https://docs.cohere.com/docs/the-attention-mechanism)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcd67d9-e3e4-42ca-9aa1-cef958f88dee",
   "metadata": {
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## EncoderLayer = attention + feedforward\n",
    "**vocab_size:** size of vocabulary (eg: 50K)  \n",
    "**d_model** (int) – the number of expected features in the encoder/decoder inputs (default=512).  \n",
    "**dim_feedforward** (int) – the dimension of the feedforward network model (default=2048).  \n",
    "  \n",
    "<img src=\"img/transformer-talk/encoder_with_tensors.png\" width=\"450\" height=\"450\">  \n",
    "<!-- Image Credits: Illustrated Transformers  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de57d0b-de3a-4009-8d56-f480af3f5dfa",
   "metadata": {
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Self attention\n",
    "contains 3 representations Query, Key, Value  \n",
    "**Query:** representation of current word that\"s compared  \n",
    "**Key:** representation of other word that\"s being compared to  \n",
    "**Value:** actual values that are scaled and added.  \n",
    "<img src=\"img/transformer-talk/self-attention-output.png\" width=\"450\">  \n",
    "*Image Credits: [Illustrated Transformers](https://jalammar.github.io/illustrated-transformer/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1914c4-ff4e-43f5-b8c0-ef2fe3ec8ab5",
   "metadata": {
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Multiheaded attention\n",
    "<img src=\"img/transformer-talk/transformer_multi-headed_self-attention-recap.png\" width=\"700\">\n",
    "<!-- self-attention-output.png -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f9fcbc-45d1-4ae5-94ca-659a0cdb03ef",
   "metadata": {
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Positional Encoding\n",
    "Positional encoding helps the model discern between same word occuring at different positions. It can be a learned parameter or can be fixed.  \n",
    "For ex:  \n",
    "I live in New Delhi.  \n",
    "New Delhi is the capital of India.     \n",
    "By using positional encoding, we assign slightly different embeddings to the \"New Delhi\".  \n",
    "<img src=\"img/transformer-talk/transformer_positional_encoding_vectors.png\" width=\"500\">  \n",
    "*Image Credits: [Illustrated Transformers](https://jalammar.github.io/illustrated-transformer/)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fccf14-55d8-4728-a2b6-138c15fa8f41",
   "metadata": {
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Residual connections and Layer normalization\n",
    "**Layer Normalization:**  It involves normalizing the inputs of a layer by *subtracting the mean and dividing by the standard deviation*.  \n",
    "It ensures that the inputs to each layer have a consistent distribution, which can aid in *faster and more stable training*.  \n",
    "**Residual connections:**  Residual connections, also known as skip connections, involve adding the input of a layer to its output.\n",
    "The depth of the Transformer model allows it to capture long-range dependencies, but deep networks can suffer from *difficulties in learning due to vanishing gradients*. Residual connections *enable the gradient to flow directly through the skip connection*, making it easier for the model to learn identity mappings and facilitating the training of deeper networks.  \n",
    "<img src=\"img/transformer-talk/transformer_resideual_layer_norm.png\" width=\"400\">  \n",
    "*Image Credits: [Illustrated Transformers](https://jalammar.github.io/illustrated-transformer/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54840af-31b6-401d-bd6e-81dde16efa25",
   "metadata": {
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## encoder stack\n",
    "**d_model** (int) – the number of expected features in the encoder/decoder inputs (default=512).  \n",
    "**nhead** (int) – the number of heads in the multiheadattention models (default=8).  \n",
    "**num_encoder_layers** (int) – the number of sub-encoder-layers in the encoder (default=6).  \n",
    "**dim_feedforward** (int) – the dimension of the feedforward network model (default=2048).  \n",
    "\n",
    "<img src=\"img/transformer-talk/transformer_encoder_decoder_stack.png\" width=\"500\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e698c48b-d3c1-43e6-9194-3e396cb638f7",
   "metadata": {
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Decoder = self attention + encoder-decoder attention + feedforward layer\n",
    "**Self attention**: Same as self attention in encoder except that it is masked and only allowed to look at previously generated tokens.  \n",
    "**encoder-decoder attention**: This layer which helps the decoder focus on appropriate places in the input sequence. Q comes from decoder. K,V vectors are taken from encoder output.  \n",
    "**feed forward network** : similar to encoder. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419c6f1b-8d34-4da4-8471-86e8ebecfe09",
   "metadata": {
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Final layer and softmax\n",
    " **Linear layer**: The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.  \n",
    "**Softmax:** The softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.\n",
    "\n",
    "<img src=\"img/transformer-talk/transformer_decoder_output_softmax.png\" width=\"500\">  \n",
    "<!-- *Image Credits:* [Illustrated Transformers](\"https://jalammar.github.io/illustrated-transformer/) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95368b31-3b58-49a5-a1c8-1b9285312529",
   "metadata": {
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Inference/training\n",
    "<img src=\"img/transformer-talk/transformer_full_view.png\" width=\"800\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daef783f-d587-4fa0-8dc2-0405077a63bd",
   "metadata": {
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## What next?\n",
    "1. [huggingface NLP course](https://huggingface.co/learn/nlp-course) - Models, tokenizers, datasets, huggingface ecosystem\n",
    "2. Explore foundation models like Bert, Bart, T5, gpt\n",
    "3. Explore newer architectures like [Mixture of Experts](https://huggingface.co/blog/moe#mixture-of-experts-explained), [Mamba](https://arxiv.org/abs/2312.00752) etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f757ae62-7cdd-4669-8702-bb724fbbcce0",
   "metadata": {
    "slide_type": "slide",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## References:  \n",
    "1. [Attention is all you need(paper)](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n",
    "2. [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n",
    "3. [Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)\n",
    "4. [The Attention mechanism](https://docs.cohere.com/docs/the-attention-mechanism)\n",
    "5. [Transformer Feed-Forward Layers Are Key-Value Memories(Paper)](https://aclanthology.org/2021.emnlp-main.446.pdf)\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
